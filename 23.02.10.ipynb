{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0b7e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8129770992366412\n",
      "accuracy2: 0.8006258957035477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "%matplotlib inline\n",
    "\n",
    "titanic_df = pd.read_csv('titanic_data')\n",
    "\n",
    "titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\n",
    "titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace=True)\n",
    "titanic_df['Cabin'].fillna('N',inplace=True)\n",
    "titanic_df['Embarked'].fillna('N',inplace=True)\n",
    "titanic_df['Fare'].fillna(0, inplace=True)\n",
    "titanic_df.drop(['PassengerId','Ticket', 'Name'], axis = 1, inplace = True)\n",
    "\n",
    "def encode_features(dataDF):\n",
    "    features = ['Cabin', 'Sex', 'Embarked']\n",
    "    le = LabelEncoder()\n",
    "    for feature in features:\n",
    "        dataDF[feature] = le.fit_transform(dataDF[feature])\n",
    "\n",
    "    titanic_df = dataDF\n",
    "\n",
    "encode_features(titanic_df)\n",
    "target_df = titanic_df['Survived']\n",
    "feature_df = titanic_df.drop('Survived', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df,\n",
    "                                                    test_size = 0.2, random_state = 11)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train, y_train)\n",
    "result = dt_clf.predict(X_test)\n",
    "print(\"accuracy:\", accuracy_score(y_test, result))\n",
    "\n",
    "kfold = KFold(n_splits = 5)\n",
    "accuracy_list = []\n",
    "\n",
    "for train_index, test_index in kfold.split(feature_df):\n",
    "    X_train, X_test = feature_df.values[train_index], feature_df.values[test_index]\n",
    "    y_train, y_test = target_df.values[train_index], target_df.values[test_index]\n",
    "    \n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    result = dt_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, result)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "\n",
    "print('accuracy2:', np.mean(accuracy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dd2eefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "[[0.27716031 0.72283969]\n",
      " [0.69768529 0.30231471]\n",
      " [0.32903767 0.67096233]\n",
      " [0.30779418 0.69220582]\n",
      " [0.86120262 0.13879738]\n",
      " [0.17742163 0.82257837]\n",
      " [0.89758424 0.10241576]\n",
      " [0.85919821 0.14080179]\n",
      " [0.36828443 0.63171557]\n",
      " [0.65552914 0.34447086]\n",
      " [0.92687259 0.07312741]\n",
      " [0.09782774 0.90217226]\n",
      " [0.27105427 0.72894573]\n",
      " [0.89739024 0.10260976]\n",
      " [0.87983257 0.12016743]\n",
      " [0.90564456 0.09435544]\n",
      " [0.88765477 0.11234523]\n",
      " [0.95761241 0.04238759]\n",
      " [0.15019338 0.84980662]\n",
      " [0.14409456 0.85590544]\n",
      " [0.76149113 0.23850887]\n",
      " [0.22442669 0.77557331]\n",
      " [0.17108075 0.82891925]\n",
      " [0.82021549 0.17978451]\n",
      " [0.70198529 0.29801471]\n",
      " [0.07277727 0.92722273]\n",
      " [0.89411971 0.10588029]\n",
      " [0.03852386 0.96147614]\n",
      " [0.85454535 0.14545465]\n",
      " [0.1679603  0.8320397 ]\n",
      " [0.91868911 0.08131089]\n",
      " [0.82787688 0.17212312]\n",
      " [0.85596913 0.14403087]\n",
      " [0.86680838 0.13319162]\n",
      " [0.69674127 0.30325873]\n",
      " [0.885941   0.114059  ]\n",
      " [0.90934596 0.09065404]\n",
      " [0.74092893 0.25907107]\n",
      " [0.90463792 0.09536208]\n",
      " [0.40992229 0.59007771]\n",
      " [0.25152658 0.74847342]\n",
      " [0.79287017 0.20712983]\n",
      " [0.31482758 0.68517242]\n",
      " [0.30148673 0.69851327]\n",
      " [0.82497131 0.17502869]\n",
      " [0.57836299 0.42163701]\n",
      " [0.15926079 0.84073921]\n",
      " [0.80142005 0.19857995]\n",
      " [0.67481335 0.32518665]\n",
      " [0.33733924 0.66266076]\n",
      " [0.7971787  0.2028213 ]\n",
      " [0.06366394 0.93633606]\n",
      " [0.88474735 0.11525265]\n",
      " [0.89726226 0.10273774]\n",
      " [0.89756661 0.10243339]\n",
      " [0.71953254 0.28046746]\n",
      " [0.40526342 0.59473658]\n",
      " [0.69528506 0.30471494]\n",
      " [0.64981585 0.35018415]\n",
      " [0.30767502 0.69232498]\n",
      " [0.80384056 0.19615944]\n",
      " [0.08374502 0.91625498]\n",
      " [0.89721359 0.10278641]\n",
      " [0.2256552  0.7743448 ]\n",
      " [0.87345127 0.12654873]\n",
      " [0.14367539 0.85632461]\n",
      " [0.87355895 0.12644105]\n",
      " [0.16668808 0.83331192]\n",
      " [0.32775773 0.67224227]\n",
      " [0.87928526 0.12071474]\n",
      " [0.30778054 0.69221946]\n",
      " [0.91836775 0.08163225]\n",
      " [0.84243352 0.15756648]\n",
      " [0.71000574 0.28999426]\n",
      " [0.08713389 0.91286611]\n",
      " [0.90353064 0.09646936]\n",
      " [0.89407281 0.10592719]\n",
      " [0.66897226 0.33102774]\n",
      " [0.87058439 0.12941561]\n",
      " [0.80655662 0.19344338]\n",
      " [0.85884945 0.14115055]\n",
      " [0.20595208 0.79404792]\n",
      " [0.10552638 0.89447362]\n",
      " [0.17401167 0.82598833]\n",
      " [0.28794508 0.71205492]\n",
      " [0.70504808 0.29495192]\n",
      " [0.89727103 0.10272897]\n",
      " [0.93062511 0.06937489]\n",
      " [0.73343432 0.26656568]\n",
      " [0.1586339  0.8413661 ]\n",
      " [0.89607394 0.10392606]\n",
      " [0.23090347 0.76909653]\n",
      " [0.35310046 0.64689954]\n",
      " [0.14642418 0.85357582]\n",
      " [0.8705538  0.1294462 ]\n",
      " [0.53373935 0.46626065]\n",
      " [0.88145239 0.11854761]\n",
      " [0.90293977 0.09706023]\n",
      " [0.89739024 0.10260976]\n",
      " [0.89411971 0.10588029]\n",
      " [0.89253532 0.10746468]\n",
      " [0.15224807 0.84775193]\n",
      " [0.87356776 0.12643224]\n",
      " [0.93118893 0.06881107]\n",
      " [0.87353422 0.12646578]\n",
      " [0.21389819 0.78610181]\n",
      " [0.23815004 0.76184996]\n",
      " [0.8047158  0.1952842 ]\n",
      " [0.8972681  0.1027319 ]\n",
      " [0.71738094 0.28261906]\n",
      " [0.89739024 0.10260976]\n",
      " [0.31487927 0.68512073]\n",
      " [0.86101157 0.13898843]\n",
      " [0.63971846 0.36028154]\n",
      " [0.89411971 0.10588029]\n",
      " [0.05544493 0.94455507]\n",
      " [0.36797198 0.63202802]\n",
      " [0.88765637 0.11234363]\n",
      " [0.19136339 0.80863661]\n",
      " [0.81374237 0.18625763]\n",
      " [0.88408215 0.11591785]\n",
      " [0.85649011 0.14350989]\n",
      " [0.78849441 0.21150559]\n",
      " [0.27653991 0.72346009]\n",
      " [0.85426924 0.14573076]\n",
      " [0.30778054 0.69221946]\n",
      " [0.25619267 0.74380733]\n",
      " [0.23736303 0.76263697]\n",
      " [0.91145164 0.08854836]\n",
      " [0.8974961  0.1025039 ]\n",
      " [0.57178935 0.42821065]\n",
      " [0.87102882 0.12897118]\n",
      " [0.89721359 0.10278641]\n",
      " [0.68561284 0.31438716]\n",
      " [0.30910511 0.69089489]\n",
      " [0.88765477 0.11234523]\n",
      " [0.74530597 0.25469403]\n",
      " [0.91075297 0.08924703]\n",
      " [0.8874161  0.1125839 ]\n",
      " [0.12823992 0.87176008]\n",
      " [0.9315158  0.0684842 ]\n",
      " [0.68139955 0.31860045]\n",
      " [0.89511562 0.10488438]\n",
      " [0.90235923 0.09764077]\n",
      " [0.74841945 0.25158055]\n",
      " [0.87275837 0.12724163]\n",
      " [0.88171753 0.11828247]\n",
      " [0.30778054 0.69221946]\n",
      " [0.21602649 0.78397351]\n",
      " [0.61065479 0.38934521]\n",
      " [0.8158176  0.1841824 ]\n",
      " [0.76141258 0.23858742]\n",
      " [0.47462419 0.52537581]\n",
      " [0.86414555 0.13585445]\n",
      " [0.86513909 0.13486091]\n",
      " [0.89738141 0.10261859]\n",
      " [0.34925981 0.65074019]\n",
      " [0.08600057 0.91399943]\n",
      " [0.24030147 0.75969853]\n",
      " [0.69265277 0.30734723]\n",
      " [0.77496367 0.22503633]\n",
      " [0.89004635 0.10995365]\n",
      " [0.86223352 0.13776648]\n",
      " [0.88746237 0.11253763]\n",
      " [0.85582394 0.14417606]\n",
      " [0.77818636 0.22181364]\n",
      " [0.71344635 0.28655365]\n",
      " [0.07032761 0.92967239]\n",
      " [0.87958842 0.12041158]\n",
      " [0.18763704 0.81236296]\n",
      " [0.72137005 0.27862995]\n",
      " [0.85079244 0.14920756]\n",
      " [0.78242415 0.21757585]\n",
      " [0.29902522 0.70097478]\n",
      " [0.60030636 0.39969364]\n",
      " [0.88765637 0.11234363]\n",
      " [0.30930832 0.69069168]\n",
      " [0.89003074 0.10996926]\n",
      " [0.608895   0.391105  ]\n",
      " [0.82788164 0.17211836]\n",
      " [0.92396905 0.07603095]\n",
      " [0.78829578 0.21170422]\n",
      " [0.88765477 0.11234523]\n",
      " [0.7703192  0.2296808 ]\n",
      " [0.90242535 0.09757465]\n",
      " [0.95616923 0.04383077]\n",
      " [0.02384411 0.97615589]\n",
      " [0.92662251 0.07337749]\n",
      " [0.24179061 0.75820939]\n",
      " [0.80408888 0.19591112]\n",
      " [0.34020546 0.65979454]\n",
      " [0.79531273 0.20468727]\n",
      " [0.19302559 0.80697441]\n",
      " [0.10177922 0.89822078]\n",
      " [0.80142005 0.19857995]\n",
      " [0.72483715 0.27516285]\n",
      " [0.90966392 0.09033608]\n",
      " [0.245594   0.754406  ]\n",
      " [0.75554113 0.24445887]\n",
      " [0.22553427 0.77446573]\n",
      " [0.89727396 0.10272604]\n",
      " [0.89411971 0.10588029]\n",
      " [0.37666791 0.62333209]\n",
      " [0.98382571 0.01617429]\n",
      " [0.20009988 0.79990012]\n",
      " [0.19302559 0.80697441]\n",
      " [0.88974272 0.11025728]\n",
      " [0.08370727 0.91629273]\n",
      " [0.53458273 0.46541727]\n",
      " [0.91202322 0.08797678]\n",
      " [0.24841673 0.75158327]\n",
      " [0.13603807 0.86396193]\n",
      " [0.79984028 0.20015972]\n",
      " [0.82989756 0.17010244]\n",
      " [0.05536938 0.94463062]\n",
      " [0.72437064 0.27562936]\n",
      " [0.86852345 0.13147655]\n",
      " [0.13659366 0.86340634]\n",
      " [0.06171779 0.93828221]\n",
      " [0.40383101 0.59616899]\n",
      " [0.78340155 0.21659845]\n",
      " [0.7333438  0.2666562 ]\n",
      " [0.94299654 0.05700346]\n",
      " [0.89411971 0.10588029]\n",
      " [0.88395402 0.11604598]\n",
      " [0.30956924 0.69043076]\n",
      " [0.3107399  0.6892601 ]\n",
      " [0.81982239 0.18017761]\n",
      " [0.21492256 0.78507744]\n",
      " [0.88207193 0.11792807]\n",
      " [0.9030589  0.0969411 ]\n",
      " [0.86974597 0.13025403]\n",
      " [0.92257867 0.07742133]\n",
      " [0.4800975  0.5199025 ]\n",
      " [0.13891419 0.86108581]\n",
      " [0.86564203 0.13435797]\n",
      " [0.87834713 0.12165287]\n",
      " [0.95616026 0.04383974]\n",
      " [0.06600728 0.93399272]\n",
      " [0.87867594 0.12132406]\n",
      " [0.11746649 0.88253351]\n",
      " [0.87645924 0.12354076]\n",
      " [0.89689373 0.10310627]\n",
      " [0.04592351 0.95407649]\n",
      " [0.88058247 0.11941753]\n",
      " [0.0789492  0.9210508 ]\n",
      " [0.60117392 0.39882608]\n",
      " [0.74168995 0.25831005]\n",
      " [0.69709048 0.30290952]\n",
      " [0.83775456 0.16224544]\n",
      " [0.63449016 0.36550984]\n",
      " [0.30780439 0.69219561]\n",
      " [0.24674074 0.75325926]\n",
      " [0.30778054 0.69221946]\n",
      " [0.09140316 0.90859684]\n",
      " [0.30447278 0.69552722]\n",
      " [0.89721359 0.10278641]\n",
      " [0.06392273 0.93607727]\n",
      " [0.91664329 0.08335671]\n",
      " [0.89721359 0.10278641]\n",
      " [0.91705734 0.08294266]]\n",
      "accuracy3: 0.9808429118773946\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "print(result1)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "pred_proba = lr_clf.predict_proba(X_test)\n",
    "print(pred_proba)\n",
    "result1 = lr_clf.predict(X_test)\n",
    "print('accuracy3:', (accuracy_score(y_test, result1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b023f35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.72283969 0.30231471 0.67096233 0.69220582 0.13879738 0.82257837\n 0.10241576 0.14080179 0.63171557 0.34447086 0.07312741 0.90217226\n 0.72894573 0.10260976 0.12016743 0.09435544 0.11234523 0.04238759\n 0.84980662 0.85590544 0.23850887 0.77557331 0.82891925 0.17978451\n 0.29801471 0.92722273 0.10588029 0.96147614 0.14545465 0.8320397\n 0.08131089 0.17212312 0.14403087 0.13319162 0.30325873 0.114059\n 0.09065404 0.25907107 0.09536208 0.59007771 0.74847342 0.20712983\n 0.68517242 0.69851327 0.17502869 0.42163701 0.84073921 0.19857995\n 0.32518665 0.66266076 0.2028213  0.93633606 0.11525265 0.10273774\n 0.10243339 0.28046746 0.59473658 0.30471494 0.35018415 0.69232498\n 0.19615944 0.91625498 0.10278641 0.7743448  0.12654873 0.85632461\n 0.12644105 0.83331192 0.67224227 0.12071474 0.69221946 0.08163225\n 0.15756648 0.28999426 0.91286611 0.09646936 0.10592719 0.33102774\n 0.12941561 0.19344338 0.14115055 0.79404792 0.89447362 0.82598833\n 0.71205492 0.29495192 0.10272897 0.06937489 0.26656568 0.8413661\n 0.10392606 0.76909653 0.64689954 0.85357582 0.1294462  0.46626065\n 0.11854761 0.09706023 0.10260976 0.10588029 0.10746468 0.84775193\n 0.12643224 0.06881107 0.12646578 0.78610181 0.76184996 0.1952842\n 0.1027319  0.28261906 0.10260976 0.68512073 0.13898843 0.36028154\n 0.10588029 0.94455507 0.63202802 0.11234363 0.80863661 0.18625763\n 0.11591785 0.14350989 0.21150559 0.72346009 0.14573076 0.69221946\n 0.74380733 0.76263697 0.08854836 0.1025039  0.42821065 0.12897118\n 0.10278641 0.31438716 0.69089489 0.11234523 0.25469403 0.08924703\n 0.1125839  0.87176008 0.0684842  0.31860045 0.10488438 0.09764077\n 0.25158055 0.12724163 0.11828247 0.69221946 0.78397351 0.38934521\n 0.1841824  0.23858742 0.52537581 0.13585445 0.13486091 0.10261859\n 0.65074019 0.91399943 0.75969853 0.30734723 0.22503633 0.10995365\n 0.13776648 0.11253763 0.14417606 0.22181364 0.28655365 0.92967239\n 0.12041158 0.81236296 0.27862995 0.14920756 0.21757585 0.70097478\n 0.39969364 0.11234363 0.69069168 0.10996926 0.391105   0.17211836\n 0.07603095 0.21170422 0.11234523 0.2296808  0.09757465 0.04383077\n 0.97615589 0.07337749 0.75820939 0.19591112 0.65979454 0.20468727\n 0.80697441 0.89822078 0.19857995 0.27516285 0.09033608 0.754406\n 0.24445887 0.77446573 0.10272604 0.10588029 0.62333209 0.01617429\n 0.79990012 0.80697441 0.11025728 0.91629273 0.46541727 0.08797678\n 0.75158327 0.86396193 0.20015972 0.17010244 0.94463062 0.27562936\n 0.13147655 0.86340634 0.93828221 0.59616899 0.21659845 0.2666562\n 0.05700346 0.10588029 0.11604598 0.69043076 0.6892601  0.18017761\n 0.78507744 0.11792807 0.0969411  0.13025403 0.07742133 0.5199025\n 0.86108581 0.13435797 0.12165287 0.04383974 0.93399272 0.12132406\n 0.88253351 0.12354076 0.10310627 0.95407649 0.11941753 0.9210508\n 0.39882608 0.25831005 0.30290952 0.16224544 0.36550984 0.69219561\n 0.75325926 0.69221946 0.90859684 0.69552722 0.10278641 0.93607727\n 0.08335671 0.10278641 0.08294266].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1556\\339370010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2099\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m         \"\"\"\n\u001b[1;32m-> 2101\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    770\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.72283969 0.30231471 0.67096233 0.69220582 0.13879738 0.82257837\n 0.10241576 0.14080179 0.63171557 0.34447086 0.07312741 0.90217226\n 0.72894573 0.10260976 0.12016743 0.09435544 0.11234523 0.04238759\n 0.84980662 0.85590544 0.23850887 0.77557331 0.82891925 0.17978451\n 0.29801471 0.92722273 0.10588029 0.96147614 0.14545465 0.8320397\n 0.08131089 0.17212312 0.14403087 0.13319162 0.30325873 0.114059\n 0.09065404 0.25907107 0.09536208 0.59007771 0.74847342 0.20712983\n 0.68517242 0.69851327 0.17502869 0.42163701 0.84073921 0.19857995\n 0.32518665 0.66266076 0.2028213  0.93633606 0.11525265 0.10273774\n 0.10243339 0.28046746 0.59473658 0.30471494 0.35018415 0.69232498\n 0.19615944 0.91625498 0.10278641 0.7743448  0.12654873 0.85632461\n 0.12644105 0.83331192 0.67224227 0.12071474 0.69221946 0.08163225\n 0.15756648 0.28999426 0.91286611 0.09646936 0.10592719 0.33102774\n 0.12941561 0.19344338 0.14115055 0.79404792 0.89447362 0.82598833\n 0.71205492 0.29495192 0.10272897 0.06937489 0.26656568 0.8413661\n 0.10392606 0.76909653 0.64689954 0.85357582 0.1294462  0.46626065\n 0.11854761 0.09706023 0.10260976 0.10588029 0.10746468 0.84775193\n 0.12643224 0.06881107 0.12646578 0.78610181 0.76184996 0.1952842\n 0.1027319  0.28261906 0.10260976 0.68512073 0.13898843 0.36028154\n 0.10588029 0.94455507 0.63202802 0.11234363 0.80863661 0.18625763\n 0.11591785 0.14350989 0.21150559 0.72346009 0.14573076 0.69221946\n 0.74380733 0.76263697 0.08854836 0.1025039  0.42821065 0.12897118\n 0.10278641 0.31438716 0.69089489 0.11234523 0.25469403 0.08924703\n 0.1125839  0.87176008 0.0684842  0.31860045 0.10488438 0.09764077\n 0.25158055 0.12724163 0.11828247 0.69221946 0.78397351 0.38934521\n 0.1841824  0.23858742 0.52537581 0.13585445 0.13486091 0.10261859\n 0.65074019 0.91399943 0.75969853 0.30734723 0.22503633 0.10995365\n 0.13776648 0.11253763 0.14417606 0.22181364 0.28655365 0.92967239\n 0.12041158 0.81236296 0.27862995 0.14920756 0.21757585 0.70097478\n 0.39969364 0.11234363 0.69069168 0.10996926 0.391105   0.17211836\n 0.07603095 0.21170422 0.11234523 0.2296808  0.09757465 0.04383077\n 0.97615589 0.07337749 0.75820939 0.19591112 0.65979454 0.20468727\n 0.80697441 0.89822078 0.19857995 0.27516285 0.09033608 0.754406\n 0.24445887 0.77446573 0.10272604 0.10588029 0.62333209 0.01617429\n 0.79990012 0.80697441 0.11025728 0.91629273 0.46541727 0.08797678\n 0.75158327 0.86396193 0.20015972 0.17010244 0.94463062 0.27562936\n 0.13147655 0.86340634 0.93828221 0.59616899 0.21659845 0.2666562\n 0.05700346 0.10588029 0.11604598 0.69043076 0.6892601  0.18017761\n 0.78507744 0.11792807 0.0969411  0.13025403 0.07742133 0.5199025\n 0.86108581 0.13435797 0.12165287 0.04383974 0.93399272 0.12132406\n 0.88253351 0.12354076 0.10310627 0.95407649 0.11941753 0.9210508\n 0.39882608 0.25831005 0.30290952 0.16224544 0.36550984 0.69219561\n 0.75325926 0.69221946 0.90859684 0.69552722 0.10278641 0.93607727\n 0.08335671 0.10278641 0.08294266].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f947f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261,)\n",
      "(261,)\n",
      "(261, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p = lr_clf.predict_proba(X_test)[:, 1]\n",
    "pred = lr_clf.predict(X_test)\n",
    "print(p.shape)\n",
    "\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)\n",
    "print(type(X_test))\n",
    "print(type(y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
